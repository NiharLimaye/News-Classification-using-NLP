{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn_evaluation.plot import confusion_matrix\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re\n",
    "import string\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('News Aggregator/uci-news-aggregator.csv', usecols=['TITLE', 'CATEGORY','PUBLISHER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e    152462\n",
       "b    115964\n",
       "t    108341\n",
       "m     45639\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M class has way less data than the orthers, thus the classes are unbalanced.\n",
    "data.CATEGORY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TITLE        0\n",
       "PUBLISHER    2\n",
       "CATEGORY     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 417046 entries, 0 to 422405\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   TITLE      417046 non-null  object\n",
      " 1   PUBLISHER  417044 non-null  object\n",
      " 2   CATEGORY   417046 non-null  object\n",
      " 3   text       417044 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data.TITLE + \" \" + data.PUBLISHER\n",
    "data.info()\n",
    "#df.text.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 417044 entries, 0 to 422405\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   TITLE      417044 non-null  object\n",
      " 1   PUBLISHER  417044 non-null  object\n",
      " 2   CATEGORY   417044 non-null  object\n",
      " 3   text       417044 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Drop null\n",
    "data= data.dropna(subset=['text'])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48340     1\n",
      "85649     1\n",
      "174166    3\n",
      "58833     1\n",
      "68375     1\n",
      "67398     1\n",
      "89442     1\n",
      "112976    2\n",
      "81146     1\n",
      "5798      0\n",
      "Name: LABEL, dtype: int64\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#I do aspire here to have balanced classes\n",
    "num_of_categories = 45639\n",
    "shuffled = data.reindex(np.random.permutation(data.index))\n",
    "e = shuffled[shuffled['CATEGORY'] == 'e'][:num_of_categories]\n",
    "b = shuffled[shuffled['CATEGORY'] == 'b'][:num_of_categories]\n",
    "t = shuffled[shuffled['CATEGORY'] == 't'][:num_of_categories]\n",
    "m = shuffled[shuffled['CATEGORY'] == 'm'][:num_of_categories]\n",
    "concated = pd.concat([e,b,t,m], ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "concated = concated.reindex(np.random.permutation(concated.index))\n",
    "concated['LABEL'] = 0\n",
    "\n",
    "#Converting categories to numbers\n",
    "concated.loc[concated['CATEGORY'] == 'e', 'LABEL'] = 0\n",
    "concated.loc[concated['CATEGORY'] == 'b', 'LABEL'] = 1\n",
    "concated.loc[concated['CATEGORY'] == 't', 'LABEL'] = 2\n",
    "concated.loc[concated['CATEGORY'] == 'm', 'LABEL'] = 3\n",
    "print(concated['LABEL'][:10])\n",
    "labels = to_categorical(concated['LABEL'], num_classes=4)\n",
    "print(labels[:10])\n",
    "if 'CATEGORY' in concated.keys():\n",
    "    concated.drop(['CATEGORY'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>text</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48340</th>\n",
       "      <td>Chrysler US sales increase at the fastest rate since 2005</td>\n",
       "      <td>Stixs News</td>\n",
       "      <td>b</td>\n",
       "      <td>Chrysler US sales increase at the fastest rate since 2005 Stixs News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85649</th>\n",
       "      <td>Fannie Mae to pay US Treasury $5.7 billion on quarterly profit</td>\n",
       "      <td>MSN Money</td>\n",
       "      <td>b</td>\n",
       "      <td>Fannie Mae to pay US Treasury $5.7 billion on quarterly profit MSN Money</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174166</th>\n",
       "      <td>Local hospital prepares for MERS outbreak</td>\n",
       "      <td>LocalNews8.com</td>\n",
       "      <td>m</td>\n",
       "      <td>Local hospital prepares for MERS outbreak LocalNews8.com</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58833</th>\n",
       "      <td>ECB Contemplates New Horizon</td>\n",
       "      <td>MarketPulse \\(blog\\)</td>\n",
       "      <td>b</td>\n",
       "      <td>ECB Contemplates New Horizon MarketPulse \\(blog\\)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68375</th>\n",
       "      <td>NYMEX crude prices dip in Asia with U.S. industry stocks data eyed</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>b</td>\n",
       "      <td>NYMEX crude prices dip in Asia with U.S. industry stocks data eyed NASDAQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     TITLE  \\\n",
       "48340            Chrysler US sales increase at the fastest rate since 2005   \n",
       "85649       Fannie Mae to pay US Treasury $5.7 billion on quarterly profit   \n",
       "174166                           Local hospital prepares for MERS outbreak   \n",
       "58833                                         ECB Contemplates New Horizon   \n",
       "68375   NYMEX crude prices dip in Asia with U.S. industry stocks data eyed   \n",
       "\n",
       "                   PUBLISHER CATEGORY  \\\n",
       "48340             Stixs News        b   \n",
       "85649              MSN Money        b   \n",
       "174166        LocalNews8.com        m   \n",
       "58833   MarketPulse \\(blog\\)        b   \n",
       "68375                 NASDAQ        b   \n",
       "\n",
       "                                                                             text  \\\n",
       "48340        Chrysler US sales increase at the fastest rate since 2005 Stixs News   \n",
       "85649    Fannie Mae to pay US Treasury $5.7 billion on quarterly profit MSN Money   \n",
       "174166                   Local hospital prepares for MERS outbreak LocalNews8.com   \n",
       "58833                           ECB Contemplates New Horizon MarketPulse \\(blog\\)   \n",
       "68375   NYMEX crude prices dip in Asia with U.S. industry stocks data eyed NASDAQ   \n",
       "\n",
       "        LABEL  \n",
       "48340       1  \n",
       "85649       1  \n",
       "174166      3  \n",
       "58833       1  \n",
       "68375       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concated.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', \n",
    "           ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', \n",
    "           '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', \n",
    "           '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', \n",
    "           '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', \n",
    "           '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', \n",
    "           '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', \n",
    "           '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', \n",
    "           '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', \n",
    "           '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', \n",
    "           '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', \n",
    "           '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', \n",
    "           '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_symbol(text):\n",
    "    text = str(text)\n",
    "    for symbol in symbols:\n",
    "        text = text.replace(symbol, '')\n",
    "    return text\n",
    "\n",
    "# remove symbols and punctuations \n",
    "concated['text'] = concated['text'].apply(lambda x: clean_symbol(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of short form words and mispellings\n",
    "short_forms_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
    "                    \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                    \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
    "                    \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                    \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                    \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                    \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                    \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                    \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n",
    "                    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
    "                    \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n",
    "                    \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "                    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n",
    "                    \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                    \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                    \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                    \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                    \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_shortforms(text):\n",
    "    clean_text = text\n",
    "    for shortform in short_forms_dict.keys():\n",
    "        if re.search(shortform, text):\n",
    "            clean_text = re.sub(shortform, short_forms_dict[shortform], text)\n",
    "    return clean_text\n",
    "\n",
    "# fix short forms\n",
    "concated['text'] = concated['text'].apply(lambda x: clean_shortforms(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = re.sub(\"[0-9]+\", \"\",s)\n",
    "    s = re.sub(' +',' ', s)        \n",
    "    return s\n",
    "\n",
    "concated['text'] = [clean_text(s) for s in concated['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated['text'] = concated['text'].str.replace('[^\\w\\s]','')# unpunctuate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>text</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48340</th>\n",
       "      <td>Chrysler US sales increase at the fastest rate since 2005</td>\n",
       "      <td>Stixs News</td>\n",
       "      <td>b</td>\n",
       "      <td>Chrysler US sales increase at the fastest rate since Stixs News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85649</th>\n",
       "      <td>Fannie Mae to pay US Treasury $5.7 billion on quarterly profit</td>\n",
       "      <td>MSN Money</td>\n",
       "      <td>b</td>\n",
       "      <td>Fannie Mae to pay US Treasury billion on quarterly profit MSN Money</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174166</th>\n",
       "      <td>Local hospital prepares for MERS outbreak</td>\n",
       "      <td>LocalNews8.com</td>\n",
       "      <td>m</td>\n",
       "      <td>Local hospital prepares for MERS outbreak LocalNewscom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58833</th>\n",
       "      <td>ECB Contemplates New Horizon</td>\n",
       "      <td>MarketPulse \\(blog\\)</td>\n",
       "      <td>b</td>\n",
       "      <td>ECB Contemplates New Horizon MarketPulse blog</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68375</th>\n",
       "      <td>NYMEX crude prices dip in Asia with U.S. industry stocks data eyed</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>b</td>\n",
       "      <td>NYMEX crude prices dip in Asia with US industry stocks data eyed NASDAQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     TITLE  \\\n",
       "48340            Chrysler US sales increase at the fastest rate since 2005   \n",
       "85649       Fannie Mae to pay US Treasury $5.7 billion on quarterly profit   \n",
       "174166                           Local hospital prepares for MERS outbreak   \n",
       "58833                                         ECB Contemplates New Horizon   \n",
       "68375   NYMEX crude prices dip in Asia with U.S. industry stocks data eyed   \n",
       "\n",
       "                   PUBLISHER CATEGORY  \\\n",
       "48340             Stixs News        b   \n",
       "85649              MSN Money        b   \n",
       "174166        LocalNews8.com        m   \n",
       "58833   MarketPulse \\(blog\\)        b   \n",
       "68375                 NASDAQ        b   \n",
       "\n",
       "                                                                           text  \\\n",
       "48340           Chrysler US sales increase at the fastest rate since Stixs News   \n",
       "85649       Fannie Mae to pay US Treasury billion on quarterly profit MSN Money   \n",
       "174166                   Local hospital prepares for MERS outbreak LocalNewscom   \n",
       "58833                             ECB Contemplates New Horizon MarketPulse blog   \n",
       "68375   NYMEX crude prices dip in Asia with US industry stocks data eyed NASDAQ   \n",
       "\n",
       "        LABEL  \n",
       "48340       1  \n",
       "85649       1  \n",
       "174166      3  \n",
       "58833       1  \n",
       "68375       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concated.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen : 23\n",
      "len(word_freqs) : 43969\n"
     ]
    }
   ],
   "source": [
    "maxlen=0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in concated[\"text\"]:\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word = [word for word in words if (len(word) >= 2 and len(word) < 14)]\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        if word in stop_words:\n",
    "            continue;\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        word_freqs[word] += 1\n",
    "    num_recs += 1\n",
    "\n",
    "print(\"maxlen :\", maxlen)\n",
    "print(\"len(word_freqs) :\", len(word_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(concated[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 49387\n"
     ]
    }
   ],
   "source": [
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB= 49319\n",
    "MAX_TITLE_LENGTH = 23\n",
    "\n",
    "vocab_size = min(MAX_VOCAB, len(word_freqs)) + 1#Somado 1 por causa do UNK\n",
    "word2index = {x[0]: i+1 for i, x in enumerate(word_freqs.most_common(MAX_VOCAB))}\n",
    "word2index[\"UNK\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for sentence in concated[\"text\"]:\n",
    "    words = nltk.word_tokenize\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word = [word for word in words if (len(word) >= 2 and len(word) < 14)]\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X.append(seqs)\n",
    "    \n",
    "for category in concated[\"LABEL\"]:\n",
    "    y.append(category)\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_TITLE_LENGTH)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tools needed and use our previously defined functions to calculate precision and recall\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE= 100\n",
    "#HIDDEN_LAYER_SIZE = 50\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCH = 5\n",
    "\n",
    "# Define Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_TITLE_LENGTH))\n",
    "model.add(LSTM(128, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "#model.add(Dense(4))\n",
    "#model.add(Activation(\"softmax\"))\n",
    "\n",
    "model_adam = model\n",
    "model_adam.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy', precision_m, recall_m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 127387 samples, validate on 54595 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e595e5770394881a204ea8bf15e3fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=5.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 0', max=127387.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 140s - loss: 0.5219 - acc: 0.7984 - precision_m: 0.7950 - recall_m: 0.8219 - val_loss: 0.2188 - val_acc: 0.9268 - val_precision_m: 0.9316 - val_recall_m: 0.9233\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=127387.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 133s - loss: 0.2033 - acc: 0.9324 - precision_m: 0.9359 - recall_m: 0.9282 - val_loss: 0.1862 - val_acc: 0.9361 - val_precision_m: 0.9336 - val_recall_m: 0.9390\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=127387.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 132s - loss: 0.1551 - acc: 0.9479 - precision_m: 0.9491 - recall_m: 0.9454 - val_loss: 0.1845 - val_acc: 0.9388 - val_precision_m: 0.9393 - val_recall_m: 0.9381\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=127387.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 132s - loss: 0.1296 - acc: 0.9558 - precision_m: 0.9596 - recall_m: 0.9495 - val_loss: 0.1842 - val_acc: 0.9384 - val_precision_m: 0.9445 - val_recall_m: 0.9327\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=127387.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 134s - loss: 0.1120 - acc: 0.9614 - precision_m: 0.9654 - recall_m: 0.9505 - val_loss: 0.1945 - val_acc: 0.9378 - val_precision_m: 0.9479 - val_recall_m: 0.9266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_adam = model_adam.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, \n",
    "                    validation_data=(Xtest, ytest), verbose=2, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using test data\n",
    "predictions=model_adam.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using train data\n",
    "predictions1=model_adam.predict(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97     13721\n",
      "           1       0.92      0.89      0.91     13767\n",
      "           2       0.92      0.92      0.92     13664\n",
      "           3       0.97      0.95      0.96     13443\n",
      "\n",
      "   micro avg       0.95      0.93      0.94     54595\n",
      "   macro avg       0.95      0.93      0.94     54595\n",
      "weighted avg       0.95      0.93      0.94     54595\n",
      " samples avg       0.92      0.93      0.92     54595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihar\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "report = classification_report(ytest, predictions.round())\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
